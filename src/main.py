#!/usr/bin/env python3
"""
LinkedIn Job Scraper - CLI Entry Point

Usage:
    python src/main.py "https://linkedin.com/jobs/view/..." [options]

Options:
    -m, --markdown    Also create a Markdown file
    -o, --output      Output directory (default: ./output)
    --no-headless     Show browser window (for debugging)

KEY CONCEPT: CLI Design
- Positional argument for the required URL
- Optional flags for behavior modification
- Sensible defaults that can be overridden
"""

import argparse
import sys
from pathlib import Path

from .scraper import create_browser, navigate_to_job
from .parser import extract_job_data
from .output import save_as_json, save_as_markdown


def main():
    """Main entry point for the CLI."""
    # Parse command line arguments
    args = parse_args()
    
    # Validate URL
    if not is_valid_linkedin_url(args.url):
        print(f"Error: Invalid LinkedIn job URL: {args.url}")
        print("Expected format: https://linkedin.com/jobs/view/...")
        sys.exit(1)
    
    print(f"üîç Scraping job: {args.url}")
    
    try:
        # Scrape the job
        job_data = scrape_job(args.url, headless=not args.no_headless)
        
        # Save outputs
        output_dir = Path(args.output)
        
        # Always save JSON
        json_path = save_as_json(job_data, output_dir)
        print(f"‚úÖ Saved JSON: {json_path}")
        
        # Optionally save Markdown
        if args.markdown:
            md_path = save_as_markdown(job_data, output_dir)
            print(f"‚úÖ Saved Markdown: {md_path}")
        
        print(f"\nüìã Job: {job_data.title} at {job_data.company_name}")
        print(f"üìç Location: {job_data.location}")
        print(f"üìÖ Posted: {job_data.posted_time}")
        
    except Exception as e:
        print(f"‚ùå Error scraping job: {e}")
        sys.exit(1)


def parse_args():
    """
    Parse command line arguments.
    
    Using argparse (Python's built-in argument parser) for:
    - Automatic help message generation
    - Type validation
    - Default values
    """
    parser = argparse.ArgumentParser(
        description="Scrape LinkedIn job postings and save as JSON/Markdown",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Scrape a job (JSON only)
    python -m src.main "https://linkedin.com/jobs/view/..."
    
    # Scrape with Markdown output
    python -m src.main "https://linkedin.com/jobs/view/..." -m
    
    # Custom output directory
    python -m src.main "https://linkedin.com/jobs/view/..." -m -o ./my-jobs
        """
    )
    
    parser.add_argument(
        "url",
        help="LinkedIn job posting URL"
    )
    
    parser.add_argument(
        "-m", "--markdown",
        action="store_true",
        help="Also create a Markdown file with YAML frontmatter"
    )
    
    parser.add_argument(
        "-o", "--output",
        default="./output",
        help="Output directory (default: ./output)"
    )
    
    parser.add_argument(
        "--no-headless",
        action="store_true",
        help="Show browser window (useful for debugging)"
    )
    
    return parser.parse_args()


def is_valid_linkedin_url(url: str) -> bool:
    """
    Validate that the URL is a LinkedIn job posting.
    
    We check for:
    - linkedin.com domain
    - /jobs/view/ path
    """
    url_lower = url.lower()
    return (
        "linkedin.com" in url_lower and
        "/jobs/view/" in url_lower
    )


def scrape_job(url: str, headless: bool = True):
    """
    Scrape a single job posting.
    
    This orchestrates the scraping process:
    1. Launch browser
    2. Navigate to URL
    3. Extract data
    4. Close browser
    
    Args:
        url: LinkedIn job URL
        headless: Whether to hide the browser window
        
    Returns:
        JobData object with extracted information
    """
    with create_browser(headless=headless) as browser:
        print("üåê Launching browser...")
        
        page = navigate_to_job(browser, url)
        print("üìÑ Page loaded, extracting data...")
        
        job_data = extract_job_data(page, url)
        
        # Close the page
        page.close()
        
        return job_data


if __name__ == "__main__":
    main()
